<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Cristian  Ramírez


  | Matrix multiplication on RISC-V low-power processor

</title>
<meta name="description" content="A simple website for personal projects.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2022/distill/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=2758253787"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', '2758253787');
</script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Matrix multiplication on RISC-V low-power processor",
      "description": "In collaboration with Jie Lei and under supervision of Enrique Quintana y Adrián Castelló.",
      "published": "May 20, 2022",
      "authors": [
        
        {
          "author": "cris",
          "authorURL": "https://crissmath.com",
          "affiliations": [
            {
              "name": "GAP, UPV",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Cristian</span>   Ramírez
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Matrix multiplication on RISC-V low-power processor</h1>
        <p>In collaboration with Jie Lei and under supervision of Enrique Quintana y Adrián Castelló.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <h2 id="full-text">Full Text</h2>

<p><a href="https://link.springer.com/article/10.1007/s11227-022-04581-6">Link to <strong>paper</strong></a>.</p>

<p><a href="/assets/pdf/poster.pdf">Link to <strong>poster</strong></a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{ramirez2022blis,
  title={A BLIS-like matrix multiplication for machine learning in the RISC-V ISA-based GAP8 processor},
  author={Ramírez, Cristian and Castellón, Adrián and Quintana-Ortí, Enrique S},
  journal={The Journal of Supercomputing},
  pages={1--10},
  year={2022},
  publisher={Springer}
}
</code></pre></div></div>
<hr />

<h2 id="introdución">Introdución</h2>

<p>Las Redes Neuronales Covolucionales (CNNs) son modelos computacionales que requieren una cantidad elevada de cálculos, tanto en
la etapa de entrenamiento como en la etapa de inferencia. Por esta razón, una de las técnicas más utilizadas para implementar
las CNNs es reordenar las operaciones desde un punto de vista computacional como una multiplicación de matrices(Gemm).
En este proyecto se realiza una implementación de la Gemm), enfocado a la convolución de Redes Neuronales, en uno de los cores
RISC-V del procesador GAP8 de GreenWaves.</p>

<p>Para esta implementación, nuestro enfoque aprovecha los algoritmos de BLIS (Basic Linear Algebra Instantiation Software)
para desarrollar una implementación que :</p>

<ol>
  <li>Reorganiza el algoritmo Gemm adaptando su micro-kernel para explotar el producto escalar soportado por el hardware en el GAP8.</li>
  <li>Orquesta explícitamente las transferencias de datos a través de la jerarquía de memorias scratchpad mediante DMA (acceso directo a memoria).</li>
  <li>Opera con aritmética de enteros(int8).</li>
</ol>

<hr />

<h2 id="importancia-de-la-multiplicación-de-matrices-en-machine-learning">Importancia de la Multiplicación de matrices en Machine Learning</h2>

<p>La multiplicación general de matrices conocida en Inglés como GEMM(General Matrix Multiplication), es una de las  operaciones fundamentales que se presentan mas a menudo en las redes neuronales,
es decir dentro de algunas de las capas mas conocidas encontramos que su funcionamiento esta basado en operaciones del tipo GEMM, AXPY o un DOT.</p>

<p>Por enumerar algunas de las capas que contienen la GEMM tendriamos :</p>
<ul>
  <li>Fully-connected.</li>
  <li>Recurrent-layers:
    <ul>
      <li>RNNs (Redes Neuronales Recursivas)</li>
      <li>LSTMs(Long short-Term Memory)</li>
      <li>GRUs(Gate recurrent Units)
y por supuesto las conocidas Redes convolucionales .</li>
    </ul>
  </li>
</ul>

<p>Como vemos casi todas :P. Si bien es cierto la multiplicación de matrices abarca un gran numero de algoritmos, iniciando por el algoritmo nativo(naive),
hasta el presentado por  <a href="https://arxiv.org/pdf/2010.05846.pdf">Alman J, Williams</a> en 2020. Lo que se ha intentado en este articulo es comprender los algoritmos desde un punto computacional,
y el rendimiento que este tiene al momento de ser implementado en diferentes arquitecturas.</p>

<h3 id="definimos-que-es-la-gemm">Definimos que es la GEMM.</h3>

<p>Como mencionamos antes,  La operación GEMM no es mas que el producto general de matrices, este se define matemáticamente como :</p>

<p>$C = \alpha A \times B + \beta C$</p>

<p>Donde :</p>
<ul>
  <li>A  y B : son matrices de entrada</li>
  <li>alpha y beta : Como estradas escalares</li>
  <li>C : la matriz donde vamos a escribir los resultados.</li>
</ul>

<p>Si ya nos estamos preguntando para que sirve alpha y beta, estas se incluyen en la mayoría de frameworks de algebra lineal
y son dos entradas que nos permiten escalar los coeficientes de las matrices,
alpha escala el producto de A*B y beta escala la matrix de salida.</p>

<h3 id="modelo-computacional">Modelo computacional</h3>

<p>Si damos un salto desde la parte teórica y abordamos los conceptos desde un punto computacional, debemos tomar en cuenta que estos conceptos llevan desarrollándose desde muchos años atrás,
existiendo una gran cantidad de Librerías especializadas para este tipo de operaciones que son fundamentales en las ciencias de la computacion.
Algunas de las Librerías de Algebra lineal mas utilizadas son OpenBLAS(Basic Linear Algebra Subprogramas), intel MKL(Math Kernel Library) y  CuBLAS de NVIDIA. En su mayoría estas librerías utilizan las siguientes convenciones :</p>

<ul>
  <li>$ A(m \times k)  $ : m : filas, k: columnas.</li>
  <li>$ B(k \times n)  $ : k : filas, n: columnas.</li>
  <li>$ C(m \times n)  $ : m : filas, n: columnas.</li>
</ul>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/assets/img/matrix_model.png" alt="" title="Multiplicacion de matrices modelo computacional" />
    </div>
</div>
<div class="caption">
    Multiplicación de matrices modelo computacional.
</div>

<hr />

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Cristian  Ramírez.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib">
  </d-bibliography>

</html>
